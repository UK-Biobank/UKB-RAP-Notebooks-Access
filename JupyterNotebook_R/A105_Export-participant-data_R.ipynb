{"cells":[{"cell_type":"markdown","id":"88d04edd-75ff-4888-b8b3-af41245a8b0a","metadata":{},"source":["## R accessing data notebook\n","\n","The purpose of this notebook is to demonstrate how to access, explore and investigate the participant and record table data.\n","\n","\n","##### Run info\n","\n","- Runtime: 20 mins\n","- Instance: mem1_hdd1_v2_x8\n","- Cost: £0.30\n","\n","### This notebook depends on\n","- **A spark instance** \n","\n","\"With Spark, only one-step is needed where data is read into memory, operations performed, and the results written back—resulting in a much faster execution. Spark also reuses data by using an in-memory cache to greatly speed up machine learning algorithms that repeatedly call a function on the same dataset\" - [Amazon Web Services](https://aws.amazon.com/what-is/apache-spark/#:~:text=With%20Spark%2C%20only%20one%2Dstep,function%20on%20the%20same%20dataset.)\n","\n","## 1. Install required packages \n","Function `p_load` from `pacman` loads packages into R. If the given package is missing p_load will automatically install it - this can take a considerable amount of time for a package that needs C or FORTRAN code compilation. The following packages are needed to run this notebook:\n","\n","*Note*: If you wish to rerun this notebook, and avoid having to wait for the installation of the packages, creating a [snapshot](https://documentation.dnanexus.com/user/jupyter-notebooks#environment-snapshotshttps://documentation.dnanexus.com/user/jupyter-notebooks#environment-snapshots) of the environment is useful.\n","\n","- `sparklyr` – Allows access to spark data and interact with using familiar R interfaces such as dplyr.\n","- `reticulate` - R-Python interface, required to use the `dxdata` package that connects to Spark database and allows retrieval of phenotypic data\n","- `Stringr` – Used for character manipulation \n","- `parallel` - Parallel computation in R\n","- `Dplyr` – Tabular data manipulation in R\n","- `data.table` – Read data into data.table format \n","- `arrow` - Input/output library for Apache binary files\n"]},{"cell_type":"code","execution_count":null,"id":"7466e43a-af9c-4a49-99db-a7e260051731","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":[" # Load required packages \n","if(!require(pacman)) install.packages(\"pacman\")\n","install.packages(\"sparklyr\")\n","pacman::p_load(sparklyr, reticulate, stringr, parallel, dplyr, data.table, arrow)"]},{"cell_type":"markdown","id":"23fc5687-cab7-4f78-bfba-5717d0009b07","metadata":{},"source":["## 2. Create spark connection and define database\n","We set a sc variable, which establishes a connection to the Spark cluster.\n","\n","To connect to the database, manipulate the database path to ensure it is in the correct format."]},{"cell_type":"code","execution_count":null,"id":"657625fa-1b7b-4476-93af-022d55763388","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# Connecting to master node to orchestrates the analysis in spark \n","port <- Sys.getenv(\"SPARK_MASTER_PORT\") # retrieve port number to establish a connection with the spark cluster\n","master <- paste(\"spark://master:\", port, sep = '')\n","sc = spark_connect(master)"]},{"cell_type":"markdown","id":"7856048e-7ea3-410d-9bbd-7968e8cc5ee4","metadata":{},"source":["To improve the reproducibility of your notebooks and ensure they are portable across projects, it is better not to hardcode any database or dataset names. Instead, you can use the following code to automatically discover the database and dataset:"]},{"cell_type":"code","execution_count":null,"id":"62ade773-f87b-49da-8da8-42f84ecff674","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# import python packages\n","dxdata <- import(\"dxdata\")\n","\n","# connect to dataset\n","project_id <- system(\"dx env | grep project- | awk -F '\\t' '{print $2}'\", intern = TRUE)\n","record_id <- system(\"dx describe *dataset | grep  record- | awk -F ' ' '{print $2}' | head -n 1\" , intern = TRUE)\n","DATASET_ID <- paste0(project_id, \":\", record_id)\n","dataset <- dxdata$load_dataset(id=DATASET_ID)\n","\n","# assign app_id and database\n","database_path <- system(\"dx find data --class database\", intern =TRUE)\n","app_substring <- na.omit(str_extract(database_path, '(app\\\\d+_\\\\d+)'))\n","database_substring <- str_extract(database_path[str_detect(database_path, app_substring)], 'database-([A-Za-z0-9]+)') %>% tolower()  %>% str_replace(\"database-\", \"database_\")\n","database <- paste0(database_substring, \"__\", app_substring)"]},{"cell_type":"markdown","id":"616418ef-966e-4db8-b3ae-1edc269dd244","metadata":{},"source":["## 3. Select the database for the session and explore tables"]},{"cell_type":"code","execution_count":null,"id":"0f7e6c0e-70d4-41a6-a4c1-0e7c057fbfb7","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["tbl_change_db(sc, paste0(database))"]},{"cell_type":"markdown","id":"0d86671b-18a8-4e95-ad07-343b83cf2892","metadata":{},"source":["The database is broken down into different tables. These tables include the participant table (e.g. demographics, blood levels, etc.), olink tables (proteomics data), OMOP tables (Observational Medical Outcomes Partnership), HESIN tables (Hospital Episode Statistics Inpatient database) etc. "]},{"cell_type":"code","execution_count":null,"id":"5d398cc3-9d42-4b85-a5af-41c15edd25dd","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# View tables within the database\n","tables <- DBI::dbGetQuery(sc, paste0(\"SHOW TABLES\"))\n","\n","# You may notice each table appearing twice, using a regular name and a versioned name, such as \"hesin_critical\" and \"hesin_critical_v18_1_edeb6b8\". \n","# This naming scheme is part of the system's architecture, supporting data refreshes and participant withdrawals.\n","# Due to this behavior, please make sure to always use the regularly named tables e.g. \"hesin_critical\" \n","tables %>% \n","    filter(str_detect(tableName, \"hesin\")) # e.g. look at Hospital Episode Statistics Inpatient database (hesin) tables"]},{"cell_type":"markdown","id":"bcab525c-7645-4394-95e1-eb5c554f03aa","metadata":{},"source":["## 4. [Accessing record tables](https://biobank.ctsu.ox.ac.uk/crystal/docs.cgi?id=3)"]},{"cell_type":"markdown","id":"92f56ec0-3c6d-4ebb-83f6-8b2549105e3f","metadata":{},"source":["#### Using sparklyr\n","\n","`sparklyr` is a useful package to work with Spark dataframes within R. \n","However, not all functions within R are accessible, and you are restricted to a limited set of functions, primarily those in dplyr, broom and DBI packages. \n","\n","More information can be found on [sparklyr](https://spark.posit.co/). You can browse the list of [functions available on sparklyr](https://rdrr.io/github/rstudio/sparklyr/man/) on the GitHub\n","\n","\n","`sdf_sql` allows querying the record table of interest. The function returns a Spark table, not a standard dataframe. \n","\n","**Note** that the Olink data is split across multiple tables. Please see the A108_Constructing-the-Olink-dataset_R notebook to access this data. "]},{"cell_type":"code","execution_count":null,"id":"6f073b4f-3169-4a58-84fa-c601fb7d4a6c","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# View columns names within a table\n","sdf_sql(sc, \"SHOW COLUMNS FROM hesin_critical\") %>% pull(col_name)"]},{"cell_type":"code","execution_count":null,"id":"14a65e85-bbbe-447e-846c-4f0dfc579091","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["hesin_critical_sdf <- sparklyr::sdf_sql(sc, \"SELECT * FROM hesin_critical\")\n","class(hesin_critical_sdf)"]},{"cell_type":"code","execution_count":null,"id":"e6ac8a3c-5694-497b-a9cb-254399c55918","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# Column data type\n","as.data.frame(sdf_schema(hesin_critical_sdf)) %>% # reads the schema of a Spark dataframe\n","    select(contains(\"type\")) %>% \n","    pivot_longer(cols = everything(), names_to = \"variable\", values_to = \"data_type\")"]},{"cell_type":"code","execution_count":null,"id":"c7fde032-f721-4cc3-8f7d-7de6e0594753","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# example of sparklyr data wrangling functions\n","hesin_critical_sdf_wrangled <- hesin_critical_sdf %>% \n","    filter(cclev3days > 0) %>%\n","    select(eid, ins_index, cclev3days, unitbedconfig)"]},{"cell_type":"code","execution_count":null,"id":"16615b9c-7873-422f-9e90-db54620807a0","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# data wrangling can also be performed in the sql query \n","hesin_critical_sdf_2 <- sparklyr::sdf_sql(sc, \"SELECT eid, ins_index, cclev3days, unitbedconfig \n","                                                FROM hesin_critical\n","                                                WHERE cclev3days > 0\")\n"]},{"cell_type":"markdown","id":"b58da6e7-4b35-4fb5-9c7f-15b39b74144f","metadata":{},"source":["##### Conversion of Spark df to R df\n","For smaller dataframes, it is possible to convert the Spark df to a standard R data.frame. This allows access to functions not available to Spark dataframes and saving as .csv to the environment."]},{"cell_type":"code","execution_count":null,"id":"16b1eb47-0ef8-4b2b-b4c3-d0c1b2d66017","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["hesin_critical_df <- hesin_critical_sdf %>% collect() # collect() allows the conversion to an R dataframe.\n","class(hesin_critical_df)"]},{"cell_type":"code","execution_count":null,"id":"c8af4e7c-1899-4bc0-a0eb-5db59bc25c07","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["write.csv(hesin_critical_df, \"hesin_critical_data.csv\")"]},{"cell_type":"markdown","id":"51ac5d3c-0246-42fd-a97e-2c1b41809938","metadata":{"tags":[]},"source":["##### Upload File Locally\n","Saving the file to the current environment (within the session) does not upload it locally to the project environment, and the file will be lost when the session terminates. \n","\n","You should upload the data saved in the local environment to the project environment if you wish to save it permanently and be able to reuse it for later analysis - this can also be uploaded into RStudio.\n","\n","This is possible with the dx command [`dx upload`](https://documentation.dnanexus.com/user/objects/uploading-and-downloading-files/small-sets-of-files/uploading-using-dx). "]},{"cell_type":"code","execution_count":null,"id":"23320232-4e1c-45a4-bf2f-12a446bac577","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["system(\"dx upload hesin_critical_data.csv\")"]},{"cell_type":"markdown","id":"643886ee-744f-48d5-af93-9b0e3ed74924","metadata":{},"source":["##### Disconnecting from the r spark connection\n","\n","This allows connection to the Pyspark, which is needed to access the phenotypic data."]},{"cell_type":"code","execution_count":null,"id":"e196dd26-df04-4602-af73-b3f3778751ff","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["spark_disconnect(sc)"]},{"cell_type":"markdown","id":"e3f03d24-adc9-483a-aea5-e4e3e152500a","metadata":{},"source":["## 5. Extract the Phenotypic data\n","\n","Because the main participant data is horizontally split into multiple tables, you may find that SQL is less suitable for querying those tables directly.\n","\n","Functions from `dxdata` and schema allow access to the phenotypic data present in the participant table.\n","\n","*Some of the features presented below can also be used to explore data record tables.* "]},{"cell_type":"code","execution_count":null,"id":"bdcacefe-591f-42ec-9765-6a617f215f18","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# Connect to Pyspark, unlike r Spark, it is currently unknown how to disconnect from Pyspark\n","# You can't be connected to Pyspark and r Spark in the same time\n","engine <- dxdata$connect(dialect=\"hive+pyspark\")"]},{"cell_type":"markdown","id":"5a57e536-2bd7-458e-9abb-fb4314a952c9","metadata":{"tags":[]},"source":["###  Select the `participant` table\n","\n","The following code selects the `participant` table. You can view the tables available in the dataset using `dataset$entities`. "]},{"cell_type":"code","execution_count":null,"id":"d8343d59-8be7-47c0-89f8-5658a3f8522d","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["dataset = dxdata$load_dataset(id=DATASET_ID)\n","participant_database = dataset[\"participant\"]"]},{"cell_type":"markdown","id":"f13d4586-6ad9-45b4-afdd-c06674ac097d","metadata":{},"source":["### Schema data\n","Importing the schema data from the Showcase metadata folder in the project allows you to explore the list of fields, and map field ids to names, making it easier to search within the data.\n","\n","Data can also be accessed with the dx command [`dx extract dataset`](https://documentation.dnanexus.com/user/helpstrings-of-sdk-command-line-utilities#extract_dataset)."]},{"cell_type":"code","execution_count":null,"id":"cce8d585-4280-42ac-a23a-22f848a6e4c8","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# Schema data, contains searchable field titles \n","field_schema <- fread(\"/mnt/project/Showcase metadata/field.tsv\")\n","\n","# download data dictionary schema into the environment, contains field ids\n","system(paste0(\"dx extract_dataset \", DATASET_ID, \" -ddd\"), intern = TRUE)\n","datadict <- data.table::fread(paste0(app_substring,\".dataset.data_dictionary.csv\"))"]},{"cell_type":"markdown","id":"fa50577f-4f01-4420-ba4f-0af8e591a890","metadata":{},"source":["##### Fields identifiers\n","\n","Within the database, fields are identified by:\n","* Field id: this correspond to the unique field integer identifier\n","* Field title: this is the title of the field \n","* Field name: this includes the entity, field id, field instance and array. \n","\n","e.g. Field 94: \n","* Field id = 94\n","* Field title = Diastolic blood pressure, manual reading\n","* Field name = 'p94_i0_a0', 'p94_i0_a1', 'p94_i1_a0', 'p94_i1_a1', 'p94_i2_a0', 'p94_i2_a1', 'p94_i3_a0', 'p94_i3_a1'\n"]},{"cell_type":"markdown","id":"047068d1-9cf8-45da-b3da-2802e68ff2c8","metadata":{"tags":[]},"source":["#### Searching for fields ids using titles"]},{"cell_type":"code","execution_count":null,"id":"d0e1787f-70cf-4ad0-be82-69accba8d855","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["field_ids_of_interest <- field_schema %>% \n","                         filter(title %in% c(\"Standing height\", \"Sex\", \"Weight method\") | # searching using titles\n","                                str_detect(title, \"blood pressure\") | # searching using regex\n","                                main_category == 1020) %>% # searching category\n","                         pull(field_id)"]},{"cell_type":"markdown","id":"d1a01bab-d8fa-4f9c-ae0e-c088472d6ae2","metadata":{},"source":["The following function allows mapping of the given list of  field ids to the corresponding field names. Note that this function is for the participant table. "]},{"cell_type":"code","execution_count":null,"id":"d7cd19af-734a-421e-9510-2ddbf5ba0d16","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# some field ids have multiple instances, this function will find all the associated field ids\n","fields_for_id <- function(field) {\n","    \n","    regex <- paste0('^p', field, '(?![0-9])')\n","    fields <- dplyr::filter(datadict, stringr::str_detect(name, regex)) %>%\n","        dplyr::pull(name)\n","    return(fields)\n","}"]},{"cell_type":"markdown","id":"0b33a7fc-efa2-4e6f-bddd-e14f6d8a413e","metadata":{},"source":["After getting the list of the fields you'd like to extract the data from, `find_fields` is used to extract them from the database."]},{"cell_type":"code","execution_count":null,"id":"6133ce3b-09ff-49ee-8cbb-a6ef1b163d3f","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["# 'eid' is added manually because it is not included within 'field_schema' which creates the list of 'field_ids_of_interest'\n","complete_field_ids <- c('eid', unlist(lapply(field_ids_of_interest, fields_for_id)))\n","\n","# 'iterate()' is needed because the 'find_fields()' output is an itorator object \n","# The resulting object is a list of `Field` objects (object specific to dxdata).\n","complete_field_ids <- iterate(participant_database$find_fields(names=complete_field_ids))"]},{"cell_type":"markdown","id":"d6807733-901d-473d-9497-9f87f7c1d17b","metadata":{"tags":[]},"source":["#### Assigning column names\n","\n","Within 'complete_fields_ids' there is an attribute `title` which provides the title of the associated field_id. This an attribute for `Field` objects.\n","\n","The data can be extracted with the function `retrieve_fields`. This functions takes as input the fields list `fields` and has the optional input `column_alias`. This argument maps the field id to its corresponding title, allowing for readable column names if desired.\n","\n","The following code removes special characters and spaces to avoid errors when using the `column_alias` argument within the `retrieve_fields` function. "]},{"cell_type":"code","execution_count":null,"id":"71899bf0-626c-48cf-b43a-3aeed13d0e41","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["complete_col_names_clean <- lapply(seq_along(complete_field_ids), function(i) {\n","    \n","    clean_title <- gsub(\" \", \"_\", complete_field_ids[[i]]$title) # replace spaces with an _\n","    clean_title <- gsub(\"[^a-zA-Z0-9_]\", \"\", clean_title) # remove special characters\n","    \n","    setNames(list(clean_title), complete_field_ids[[i]]$column_name)\n","\n","})"]},{"cell_type":"markdown","id":"b6f95959-8435-47f4-8687-3d9ba082b3e5","metadata":{},"source":["`complete_col_names_clean()` is a nested list, `do.call` flattens the list.\n","\n","`dict()` creates a python dictionary which is the data type used for the `column_alias` arguement within the `retrieve_fields()` function. "]},{"cell_type":"code","execution_count":null,"id":"382098f1-06b4-43c9-9257-85d71d63cd0d","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["complete_col_names_clean <- do.call(c, complete_col_names_clean)\n","complete_col_names_clean <- dict(complete_col_names_clean)"]},{"cell_type":"markdown","id":"b09ddf77-0d1a-4c28-82ff-6dbff1073302","metadata":{"tags":[]},"source":["#### Extracting data\n","\n","Using `retrieve_fields` to access the fields of interest\n","\n","**Note** other arguements are available \n","- `coding_values` - inputs \"raw\"/\"exclude\"/\"replace\"\n","- `limit` - integer value - Maximum number of table rows to return"]},{"cell_type":"code","execution_count":null,"id":"d39f8c2e-0fe2-4cb9-89d2-65268529ec09","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["data_of_interest <- participant_database$retrieve_fields(engine=engine, fields=complete_field_ids, column_aliases = complete_col_names_clean) # returns a pyspark df"]},{"cell_type":"markdown","id":"c3cd07d1-6269-4472-878e-d39f2bff481f","metadata":{},"source":["**Side note:**  Data record tables can also be accessed using the retrieve_fields() function from dxdata \n"]},{"cell_type":"code","execution_count":null,"id":"f5a6485f-062a-403a-aa65-c50241dc78a5","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["omop_death = dataset[\"omop_death\"]$retrieve_fields(engine) "]},{"cell_type":"markdown","id":"215a5b09-9e68-41a6-88ff-36c448292dad","metadata":{"tags":[]},"source":["## 6. Write the data to a temporary `parquet` file \n","You can learn more about the _parquet_ file format here: [https://parquet.apache.org/](https://parquet.apache.org/)"]},{"cell_type":"code","execution_count":null,"id":"6098d975-5bd3-4e8b-ae20-00dbb5b17ae4","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["system('hadoop fs -rm -r -f data_of_interest.parquet', intern = TRUE)\n","data_of_interest$write$parquet('data_of_interest.parquet')"]},{"cell_type":"markdown","id":"cfbfb61f-4910-42ca-b5a5-77d30662c017","metadata":{},"source":["#### Copy the temporary _parquet_ file from distributed to the local file system"]},{"cell_type":"code","execution_count":null,"id":"d4d52dd6-a275-49b6-a2d8-f9315305a0c4","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["if(dir.exists('data_of_interest.parquet')) unlink(\"data_of_interest.parquet\", recursive=TRUE)\n","system('hadoop dfs -copyToLocal data_of_interest.parquet', intern = TRUE)\n","\n","# This file can be uploaded to your project using system(\"dx upload -r data_of_interest.parquet\") \n","# This is useful if you have a large file and want to save some steps. You can download it later in another session and use open_dataset() and collect() within RStudio for instance."]},{"cell_type":"markdown","id":"3d60f705-a9c1-482b-b853-d07018d5209c","metadata":{},"source":["#### Read the dataset information R using Apache `arrow` package"]},{"cell_type":"code","execution_count":null,"id":"47af7570-6590-4e8a-9b07-60378fbdbecb","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["data_of_interest_ds <- arrow::open_dataset('data_of_interest.parquet')"]},{"cell_type":"markdown","id":"a97b8a50-4dd1-4ae9-92a0-9c73da54d978","metadata":{"tags":[]},"source":["## Collect the data from the dataset to R memory\n","Use `collect()` to convert the phenotypic data to a standard `tibble` object, which can be interacted with using functions from the `tidyverse` package."]},{"cell_type":"code","execution_count":null,"id":"be98fd99-b2dc-473e-8d56-049b136869ca","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["data_of_interest_tbl <- data_of_interest_ds %>% collect()"]},{"cell_type":"markdown","id":"a80e4c75-a488-4a23-bf51-600c7d8715b7","metadata":{"tags":[]},"source":["#### *Optional* - rename column names to raw titles"]},{"cell_type":"code","execution_count":null,"id":"954feebe-67d0-4d64-9b0b-bcada06b1839","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["complete_col_names_raw <- lapply(seq_along(complete_field_ids), function(i) {\n","        \n","    setNames(list(complete_field_ids[[i]]$title), \"col_title\")\n","})\n","\n","complete_col_names_raw <- unlist(complete_col_names_raw, use.names = FALSE)\n","\n","names(data_of_interest_tbl) <- complete_col_names_raw"]},{"cell_type":"markdown","id":"71f90b6e-ac83-4d42-a886-a584f524eb76","metadata":{},"source":["### Write a csv in your local environment and then upload the data to you project\n","\n","This may take some time - depending on how large your data is."]},{"cell_type":"code","execution_count":null,"id":"5d7bbf0f-3d1f-4ecb-97a5-c34d36fcd7c8","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["write.csv(data_of_interest_tbl, 'data_of_interest_tbl.csv')"]},{"cell_type":"code","execution_count":null,"id":"11e15d5b-af8c-4943-be38-ca3f3b254df5","metadata":{"tags":[],"trusted":true,"vscode":{"languageId":"r"}},"outputs":[],"source":["system(\"dx upload data_of_interest_tbl.csv\")"]},{"cell_type":"markdown","id":"06d756f9","metadata":{"vscode":{"languageId":"r"}},"source":["### Export data in PLINK phenotype format"]},{"cell_type":"code","execution_count":null,"id":"177ed545","metadata":{"vscode":{"languageId":"r"}},"outputs":[],"source":["pheno_out <- data_of_interest_tbl %>% \n","    transmute(\n","        FID=`Participant ID`, \n","        IID=`Participant ID`, \n","        Y1=as.double(`Standing height | Instance 0`)\n",")\n","\n","pheno_out %>% write_delim(file = 'ukb_phenotypes_height.txt', delim = ' ')"]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.3.2"}},"nbformat":4,"nbformat_minor":5}

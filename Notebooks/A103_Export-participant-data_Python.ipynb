{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export participant data tabular formats\n",
    "\n",
    "> Scope: Retrieve fields from phenotypic table and export them to CSV and XLSX files\n",
    "\n",
    "- runtime: 10min \n",
    "- recommended instance: mem1_ssd1_v2_x8\n",
    "- cost: <Â£0.10\n",
    "\n",
    "This notebook depends on:\n",
    "* **A Spark instance**\n",
    "\n",
    "In this notebook, we will combine commands and routines from previous notebooks.\n",
    "The goal is to find and retrieve selected fields from the phenotype database. \n",
    "Then we will export this data to CSV and Excel files - this is useful for further analyses and necessary for creating input files for tools like PLINK and regenie.\n",
    "\n",
    "Further information can be found in Research Analysis Platform documentation: https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import `dxdata` package and initialize Spark engine\n",
    "### Docs at: https://github.com/dnanexus/OpenBio/blob/master/dxdata/getting_started_with_dxdata.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dxdata\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Initialize dxdata engine\n",
    "engine = dxdata.connect(dialect=\"hive+pyspark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the dataset\n",
    "\n",
    "Next, we can set a `DATASET_ID` variable, which takes a value: `[projectID]:[dataset ID]`\n",
    "We use it to define the `dataset` with `dxdata.load_dataset` function.\n",
    "\n",
    "**projectID** and **dataset ID** values are unique to your project.\n",
    "Notebook example **101** explains how to get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project = os.popen(\"dx env | grep project- | awk -F '\\t' '{print $2}'\").read().rstrip()\n",
    "record = os.popen(\"dx describe *dataset | grep  record- | awk -F ' ' '{print $2}'\").read().rstrip().split('\\n')[0]\n",
    "DATASET_ID = project + \":\" + record\n",
    "dataset = dxdata.load_dataset(id=DATASET_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieve data from the table\n",
    "\n",
    "The following code selects the `participant` table.\n",
    "Then we can define which field we are interested in using the `find_field` function.\n",
    "\n",
    "There are three main ways to identify the field of interest:\n",
    "\n",
    "- With `name` argument: here we give field ID. We can construct filed ID used by `dxdata` package from the field ID defined by UKB Showcase. The numeric showcase ID is translated to the Spark DB column name by adding the letter `p` at the beginning: e.g. *Standing height* showcase id is `50`, so Spark ID would be `p50`. Usually, fields have multiple instances. In such case, we add the `_i` suffix followed by instance number, e.g. *Standing height | Instance 0* will be `p50_i0`\n",
    "- With `title` argument: here we define the filed by full title, followed by ` | Instance` suffix, e.g. `Age at recruitment` or `Standing height | Instance 0`\n",
    "- With `title_regex` argument: here we define the filed by [regular expression](https://docs.python.org/3/howto/regex.html) matching the part of the title. We can use a keyword here, e.g. `.*height.*` will return all columns with the word *height* in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pheno = dataset['participant']\n",
    "\n",
    "# Find by field name\n",
    "field_eid = pheno.find_field(name=\"eid\")\n",
    "\n",
    "# Find by exact title\n",
    "field_sex = pheno.find_field(title=\"Sex\")\n",
    "field_age = pheno.find_field(title=\"Age at recruitment\")\n",
    "field_height = pheno.find_field(title=\"Standing height | Instance 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build a `field_list` array, comprising the fields we defined in our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Field \"eid\">, <Field \"p50_i0\">, <Field \"p31\">, <Field \"p21022\">]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_list = [field_eid, field_height, field_sex, field_age]\n",
    "field_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `retrieve_fields` build Spark DataFrame that reference data in the Spark database.\n",
    "We pass three parameters: \n",
    "- `engine` - Spark engine defined before\n",
    "- `fields` - here we provide our filed list\n",
    "- `coding_values` - the `replace` parameter here means that a function will decode any codings and replace numeric codes with informative text labels\n",
    "\n",
    "> Note: Decoding values might fail for complex, hierarchical coding, setting all values to `null`. In such a case set the parameter to `raw` to retrieve numeric codes, and then write an appropriate routine to convert these to text labels. See **Notebook 202** for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[eid: string, p50_i0: double, p31: string, p21022: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pheno_data = pheno.retrieve_fields(engine=engine, fields=field_list, coding_values=\"replace\")\n",
    "pheno_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data to local memory\n",
    "\n",
    "Calling function `toPandas()` on Spark DataFrame object will do the following;\n",
    "- collect the values from the Spark cluster to local memory\n",
    "- convert them to [Pandas](https://pandas.pydata.org/) data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_tab = pheno_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns and preview the data\n",
    "\n",
    "Column names in `data_tab` are set to field ID, e.g. `p50_i0` for height.\n",
    "Before exporting the table we can rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_tab.columns = [\"eid\", \"height\", \"sex\", \"age\"]\n",
    "data_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the table. For example, function `head()` allows print out top 5 rows of data: `data_tab.head()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data to tabular text files \n",
    "\n",
    "Now, we can save the data for further analysis. `to_csv` saves data to comma-separated values file (NB `to_tdf` saves them to tab-delimited format, which is used by most external tools, for example, PLINK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tab.to_csv('pheno_height_sex_age_500k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data to Excel format \n",
    "\n",
    "Finally, we can export data to Excel XLSX format. First, we need to install `openpyxl`: a Python library that adds support for this format to Pandas. Next, we import it and run `to_excel`, which outputs data to XLSX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tab.to_excel('pheno_height_sex_age_500k.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Excel and CSV files to your project\n",
    "\n",
    "We will use this Excel for further analyses in **Notebook 203** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "dx mkdir pheno\n",
    "dx upload pheno_height_sex_age_500k.* --path pheno/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "be0617d24f23f3f0ff0f78cfff875dd0cc8ce9ddccca39efd47dcbfb80ba815b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
